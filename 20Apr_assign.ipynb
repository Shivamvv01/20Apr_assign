{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88403ee2-3cfa-4cea-8139-2f68917afc02",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245080f3-0719-4b5d-ad9a-3702a6de0cd6",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple machine learning technique used for classification and regression tasks. It makes predictions by finding the k-nearest data points in the training dataset to a new data point and using the majority class (for classification) or the average value (for regression) of those neighbors to predict the target value for the new data point. KNN is easy to understand and implement but can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57273d-5e6a-4399-99f4-aa2bf83712c7",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5020e5-5bc6-4213-b8af-92cf4ccf95fc",
   "metadata": {},
   "source": [
    "The value of K in KNN can be choosen by:\n",
    "\n",
    "Smaller K values (e.g., 1, 3, 5) make the model sensitive to noise, potentially leading to overfitting. They capture fine-grained patterns but may not generalize well. Larger K values (e.g., 10, 20, or more) smooth the decision boundary, making the model less sensitive to noise, but they can underfit if the data has complex patterns, capturing more global trends.\n",
    "\n",
    "Preferably choosing an odd value for K in binary classification to avoid ties when voting for the majority class, ensuring a clear winner. For multiclass classification, consider the number of classes and the potential for ties when deciding whether to use an odd or even K.\n",
    "\n",
    "Using cross-validation to evaluate K's performance on a validation set.\n",
    "\n",
    "Trying a range of K values and selecting the one that results in the best model performance (e.g., accuracy for classification, mean squared error for regression).\n",
    "\n",
    "Being mindful of computational resources when selecting K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6484a7-f581-4475-a787-a03e10e884a9",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60e5e3-27eb-4018-9b84-56184b23a409",
   "metadata": {},
   "source": [
    "In classification tasks, the user seeks to predict a category, which is usually represented as an integer label, but represents a category of \"things\". For instance, you could try to classify pictures between \"cat\" and \"dog\" and use label 0 for \"cat\" and 1 for \"dog\".\n",
    "\n",
    "The KNN algorithm for classification will look at the k nearest neighbours of the input you are trying to make a prediction on. It will then output the most frequent label among those k examples.\n",
    "\n",
    "In regression tasks, the user wants to output a numerical value (usually continuous). It may be for instance estimate the price of a house, or give an evaluation of how good a movie is.\n",
    "\n",
    "In this case, the KNN algorithm would collect the values associated with the k closest examples from the one you want to make a prediction on and aggregate them to output a single value. usually, you would choose the average of the k values of the neighbours, but you could choose the median or a weighted average (or actually anything that makes sense to you for the task at hand).\n",
    "\n",
    "For your specific problem, you could use both but regression makes more sense to me in order to predict some kind of a \"matching percentage \"\"between the user and the thing you want to recommand to him."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d831a23-c885-4031-80b3-1bfc95e8bfc0",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbea1f-7408-438e-882e-0c24346f64d5",
   "metadata": {},
   "source": [
    "The performance of K-Nearest Neighbors (KNN) using the following metrics:\n",
    "\n",
    "For KNN Classification:\n",
    "\n",
    "Accuracy for overall correctness.\n",
    "Precision for the ratio of true positives to positive predictions.\n",
    "Recall for the ratio of true positives to actual positives.\n",
    "F1-Score for a balance between precision and recall.\n",
    "Confusion Matrix for detailed classification results.\n",
    "ROC Curve and AUC for binary classification.\n",
    "For KNN Regression:\n",
    "\n",
    "Mean Squared Error (MSE) for average squared prediction errors.\n",
    "Root Mean Squared Error (RMSE) for interpretable error in the same units.\n",
    "Mean Absolute Error (MAE) for average absolute prediction errors.\n",
    "R-squared (R2) for explaining the variance in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9b72c-8ac6-43de-b0cd-5b74774ac991",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4842a-3e47-49cc-9cad-ffb8ff369d3d",
   "metadata": {},
   "source": [
    "The curse of dimensionality in K-Nearest Neighbors (KNN) refers to the challenges and limitations that arise when dealing with high-dimensional data. In high-dimensional spaces, the volume increases exponentially as the number of dimensions grows, resulting in sparse data and making it difficult to find meaningful nearest neighbors. This sparsity leads to increased computational complexity, reduced discriminative power, and a need for larger datasets to maintain data density. As a result, KNN may perform poorly in high-dimensional settings, requiring careful feature selection, dimensionality reduction, or alternative distance metrics to mitigate these issues.\n",
    "\n",
    "In short as the number of dimensions increases (ie,number of features) in a dataset only upto a certain number of features the increase in accuracy is observed once there threshold number of dimensions is crossed the the accuracy of the model then decreases with the increase in number of features,this is called curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517b828-e53f-46da-98f7-85faf284ef4c",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e2599-b4a4-4cd3-8ae5-61d0b55641f7",
   "metadata": {},
   "source": [
    "To handle missing values in K-Nearest Neighbors (KNN) we use these methods :\n",
    "\n",
    "Remove Instances: If only a few instances have missing values, consider removing them if data loss is acceptable.\n",
    "\n",
    "Impute with Mean/Median: Fill missing values with the mean (for numeric data) or median (robust to outliers) of the respective feature.\n",
    "\n",
    "KNN Imputation: Use KNN to estimate missing values by averaging values from the K nearest neighbors for each missing data point.\n",
    "\n",
    "Predictive Models: Train predictive models to predict missing values based on other features.\n",
    "\n",
    "Multiple Imputation: Create multiple imputed datasets with different imputed values and analyze them separately to account for uncertainty.\n",
    "\n",
    "Weighted Distances: Assign different feature weights when calculating distances in KNN to reduce the impact of missing values in less important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51155968-aab1-49fc-8ba3-a0aa006c83ed",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9681d-14fd-4f29-88cc-038bd228b257",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) classifier and regressor are two variations of the KNN algorithm, each suited for different types of machine learning tasks.\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "The KNN classifier is designed for classification tasks, where the goal is to predict discrete class labels for data points.\n",
    "It works by assigning a data point to the majority class among its K-nearest neighbors, where K is a user-defined hyperparameter.\n",
    "KNN classifiers are effective when dealing with categorical or nominal target variables.\n",
    "They calculate distances (typically Euclidean) between data points in feature space and classify the data point based on the most common class among its neighbors.\n",
    "KNN classifiers are sensitive to the choice of K and the distance metric used.\n",
    "They are suitable for tasks like text classification, image classification, disease diagnosis, and fraud detection. However, they can be sensitive to class imbalance and may require class balancing techniques like oversampling or undersampling.\n",
    "KNN Regressor:\n",
    "\n",
    "The KNN regressor is designed for regression tasks, where the objective is to predict continuous numerical values for data points.\n",
    "It also uses a user-defined K to identify the K-nearest neighbors of a data point.\n",
    "Instead of class labels, the KNN regressor predicts the target variable's value as the average (or weighted average) of the target values of its neighbors.\n",
    "KNN regressors are sensitive to the choice of K and the distance metric, just like classifiers.\n",
    "They are well-suited for tasks like house price prediction, stock price forecasting, and temperature prediction. However, they are sensitive to outliers and may require preprocessing techniques to handle them effectively.\n",
    "Choosing Between KNN Classifier and Regressor: The choice between KNN classifier and regressor depends on the nature of the target variable and the problem context. If the target variable represents discrete categories or classes, the KNN classifier is the natural choice. Conversely, if the target variable is continuous and represents numerical values, the KNN regressor should be used. It's essential to consider the characteristics of the data and the specific objectives of the machine learning task when deciding between the two variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222e5c8-415d-4e7e-b7cc-d9f57687fc04",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ce89c-0e78-4422-b1e6-f135bf25e57d",
   "metadata": {},
   "source": [
    "Strengths\n",
    "Simplicity: Easy to understand and implement.\n",
    "Versatility: Applicable to both classification and regression tasks.\n",
    "Adaptability: Can capture complex decision boundaries.\n",
    "Weaknesses\n",
    "Computational Complexity: Can be slow for large datasets.\n",
    "Sensitivity to Noise and Outliers: Prone to noisy data and outliers.\n",
    "Impact of Irrelevant Features: Treats all features equally.\n",
    "Hyperparameter Sensitivity: Choice of K and distance metric crucial.\n",
    "Addressing Weaknesses:\n",
    "Dimensionality Reduction: Use PCA or feature selection to reduce dimensionality.\n",
    "Outlier Handling: Apply outlier detection and removal techniques.\n",
    "Feature Engineering: Carefully select and engineer relevant features.\n",
    "Distance Metric Selection: Experiment with different distance metrics.\n",
    "Class Balancing: Employ class balancing methods for imbalanced data.\n",
    "Cross-Validation: Use cross-validation for hyperparameter tuning.\n",
    "Ensemble Methods: Combine KNN models or use ensemble techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83b872-801f-43ea-a7fa-be44a82d4718",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf04949-50e2-415e-98d6-bfdfa0aa3581",
   "metadata": {},
   "source": [
    "| Formula | d = √[(x2-x1)^2 - (y2-y1)^2]| d = [mod(x2-x1) + mod(y2-y1)] | Calculation | Considers straight-line distance. | Considers the sum of horizontal and vertical distances (city block distance). | | Sensitivity to Scale | Sensitive to scale differences. | Less sensitive to scale differences. | | Dimensionality | Works well in lower-dimensional spaces. | Effective in high-dimensional spaces. | | Performance | May perform better when features have a similar scale. | May perform better when features have different scales or in scenarios with sparse data. | | Geometry Interpretation| Represents the shortest path between two points in a Euclidean space. | Represents the distance traveled when navigating a grid-like city block network. | | Use Cases | Commonly used for spatial data, image processing, and when the relationship between features is roughly isotropic (equal in all directions). | Suitable for cases where the distance metric should reflect the effort or time to travel in a grid-like network, e.g., route planning or feature engineering in text mining. |\n",
    "\n",
    "Euclidean distance is suitable when features have similar scales and the relationship between them is isotropic, while Manhattan distance is effective in scenarios with differing feature scales or when you want to consider grid-like movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda66ee-4142-440e-a61d-25d6f4e9b036",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828132ac-f648-449c-87e1-3574e163e2b1",
   "metadata": {},
   "source": [
    "The role of feature scaling in K-Nearest Neighbors (KNN) are:\n",
    "\n",
    "Equal Contribution: Feature scaling ensures all features contribute equally to distance calculations.\n",
    "\n",
    "Distance Metric Consistency: Scaling makes distance metrics meaningful and unbiased.\n",
    "\n",
    "Convergence Improvement: It can help KNN converge faster, especially in high dimensions.\n",
    "\n",
    "Robustness to Outliers: Scaling can make KNN more robust to extreme values.\n",
    "\n",
    "Effective in High Dimensions: Especially important in high-dimensional datasets.\n",
    "\n",
    "Common Scaling Methods: Include Min-Max scaling, Z-score scaling, and robust scaling.\n",
    "\n",
    "Feature scaling is essential to ensure KNN provides accurate and unbiased results across various datasets and dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
